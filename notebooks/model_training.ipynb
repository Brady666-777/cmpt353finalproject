{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8aebe576",
   "metadata": {},
   "source": [
    "# VancouverPy: Restaurant Success Prediction Model Training\n",
    "\n",
    "This notebook contains the machine learning pipeline for predicting restaurant success scores based on location and environmental factors.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Data Loading and Exploration](#data-loading)\n",
    "2. [Exploratory Data Analysis](#eda)\n",
    "3. [Feature Selection and Engineering](#feature-engineering)\n",
    "4. [Model Training and Evaluation](#model-training)\n",
    "5. [Model Interpretation](#interpretation)\n",
    "6. [Prediction and Visualization](#prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ff316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "# Machine Learning imports\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import xgboost as xgb\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfe63b7",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Exploration {#data-loading}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47918e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data\n",
    "try:\n",
    "    df_restaurants = pd.read_csv('../data/processed/restaurants_with_features.csv')\n",
    "    X = pd.read_csv('../data/processed/model_features.csv')\n",
    "    feature_names = pd.read_csv('../data/processed/feature_names.csv')['feature'].tolist()\n",
    "    \n",
    "    print(f\"Dataset loaded successfully!\")\n",
    "    print(f\"Number of restaurants: {len(df_restaurants)}\")\n",
    "    print(f\"Number of features: {len(feature_names)}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Processed data not found. Please run the data collection and processing scripts first.\")\n",
    "    print(\"Run: python src/01_get_data.py\")\n",
    "    print(\"Then: python src/02_clean_and_feature_engineer.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c38873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data exploration\n",
    "print(\"Dataset Info:\")\n",
    "print(f\"Shape: {df_restaurants.shape}\")\n",
    "print(f\"\\nColumns: {list(df_restaurants.columns)}\")\n",
    "print(f\"\\nFeature Names: {feature_names}\")\n",
    "\n",
    "# Display first few rows\n",
    "df_restaurants.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fca5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = df_restaurants.isnull().sum()\n",
    "missing_percentage = (missing_values / len(df_restaurants)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_values,\n",
    "    'Missing Percentage': missing_percentage\n",
    "})\n",
    "\n",
    "print(\"Missing Values Summary:\")\n",
    "print(missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813f12c6",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis {#eda}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5072f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Success Score distribution\n",
    "if 'success_score' in df_restaurants.columns:\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(df_restaurants['success_score'], bins=30, alpha=0.7, edgecolor='black')\n",
    "    plt.title('Distribution of Success Scores')\n",
    "    plt.xlabel('Success Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.boxplot(df_restaurants['success_score'])\n",
    "    plt.title('Success Score Box Plot')\n",
    "    plt.ylabel('Success Score')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Success Score Statistics:\")\n",
    "    print(df_restaurants['success_score'].describe())\n",
    "else:\n",
    "    print(\"Success score not found in dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2f6cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature correlations\n",
    "if len(X.columns) > 1:\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = X.corr()\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "                square=True, fmt='.2f', cbar_kws={\"shrink\": .8})\n",
    "    plt.title('Feature Correlation Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find highly correlated features\n",
    "    high_corr_pairs = []\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            if abs(corr_matrix.iloc[i, j]) > 0.8:\n",
    "                high_corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))\n",
    "    \n",
    "    if high_corr_pairs:\n",
    "        print(\"Highly correlated feature pairs (|correlation| > 0.8):\")\n",
    "        for pair in high_corr_pairs:\n",
    "            print(f\"{pair[0]} - {pair[1]}: {pair[2]:.3f}\")\n",
    "    else:\n",
    "        print(\"No highly correlated feature pairs found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9c9e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geographic distribution of restaurants\n",
    "if 'latitude' in df_restaurants.columns and 'longitude' in df_restaurants.columns:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Create scatter plot with success score as color\n",
    "    if 'success_score' in df_restaurants.columns:\n",
    "        scatter = plt.scatter(df_restaurants['longitude'], df_restaurants['latitude'], \n",
    "                            c=df_restaurants['success_score'], cmap='viridis', alpha=0.6, s=30)\n",
    "        plt.colorbar(scatter, label='Success Score')\n",
    "    else:\n",
    "        plt.scatter(df_restaurants['longitude'], df_restaurants['latitude'], alpha=0.6, s=30)\n",
    "    \n",
    "    plt.title('Geographic Distribution of Restaurants in Vancouver')\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Coordinate ranges:\")\n",
    "    print(f\"Latitude: {df_restaurants['latitude'].min():.4f} to {df_restaurants['latitude'].max():.4f}\")\n",
    "    print(f\"Longitude: {df_restaurants['longitude'].min():.4f} to {df_restaurants['longitude'].max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366a5a10",
   "metadata": {},
   "source": [
    "## 3. Feature Selection and Engineering {#feature-engineering}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ea136a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare target variable\n",
    "if 'success_score' in df_restaurants.columns:\n",
    "    y = df_restaurants['success_score'].copy()\n",
    "    print(f\"Target variable prepared. Shape: {y.shape}\")\n",
    "    print(f\"Target statistics: Mean={y.mean():.3f}, Std={y.std():.3f}\")\n",
    "else:\n",
    "    print(\"Error: Success score not found. Creating a dummy target for demonstration.\")\n",
    "    y = np.random.normal(0.5, 0.2, len(df_restaurants))\n",
    "    y = np.clip(y, 0, 1)  # Ensure values are between 0 and 1\n",
    "\n",
    "# Ensure X and y have same length\n",
    "min_length = min(len(X), len(y))\n",
    "X = X.iloc[:min_length].copy()\n",
    "y = y.iloc[:min_length] if hasattr(y, 'iloc') else y[:min_length]\n",
    "\n",
    "print(f\"Final dataset shape: X={X.shape}, y={len(y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18848630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance using Random Forest\n",
    "if len(X.columns) > 1 and len(y) > 10:\n",
    "    # Quick feature importance analysis\n",
    "    rf_temp = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf_temp.fit(X, y)\n",
    "    \n",
    "    # Create feature importance dataframe\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': rf_temp.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=feature_importance, x='importance', y='feature')\n",
    "    plt.title('Feature Importance (Random Forest)')\n",
    "    plt.xlabel('Importance Score')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Feature Importance Ranking:\")\n",
    "    print(feature_importance)\n",
    "else:\n",
    "    print(\"Insufficient data for feature importance analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba5dfb9",
   "metadata": {},
   "source": [
    "## 4. Model Training and Evaluation {#model-training}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760f2d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "if len(X) > 20:  # Minimum viable dataset size\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set: {X_train.shape}\")\n",
    "    print(f\"Testing set: {X_test.shape}\")\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    print(\"Data splitting and scaling completed!\")\n",
    "else:\n",
    "    print(f\"Dataset too small for training ({len(X)} samples). Need at least 20 samples.\")\n",
    "    X_train = X_test = y_train = y_test = None\n",
    "    X_train_scaled = X_test_scaled = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf6a79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multiple models\n",
    "models = {}\n",
    "results = {}\n",
    "\n",
    "if X_train is not None:\n",
    "    # Define models to train\n",
    "    model_configs = {\n",
    "        'Linear Regression': LinearRegression(),\n",
    "        'Ridge Regression': Ridge(alpha=1.0),\n",
    "        'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "        'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "        'XGBoost': xgb.XGBRegressor(n_estimators=100, random_state=42)\n",
    "    }\n",
    "    \n",
    "    # Train and evaluate each model\n",
    "    for name, model in model_configs.items():\n",
    "        print(f\"Training {name}...\")\n",
    "        \n",
    "        try:\n",
    "            # Use scaled data for linear models, original for tree-based\n",
    "            if 'Linear' in name or 'Ridge' in name:\n",
    "                model.fit(X_train_scaled, y_train)\n",
    "                y_pred = model.predict(X_test_scaled)\n",
    "            else:\n",
    "                model.fit(X_train, y_train)\n",
    "                y_pred = model.predict(X_test)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            rmse = np.sqrt(mse)\n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            \n",
    "            # Store results\n",
    "            results[name] = {\n",
    "                'MSE': mse,\n",
    "                'RMSE': rmse,\n",
    "                'MAE': mae,\n",
    "                'R²': r2\n",
    "            }\n",
    "            \n",
    "            models[name] = model\n",
    "            \n",
    "            print(f\"{name} - R²: {r2:.3f}, RMSE: {rmse:.3f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error training {name}: {e}\")\n",
    "    \n",
    "    print(\"\\nModel training completed!\")\n",
    "else:\n",
    "    print(\"Skipping model training due to insufficient data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab8f40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performance\n",
    "if results:\n",
    "    results_df = pd.DataFrame(results).T\n",
    "    results_df = results_df.sort_values('R²', ascending=False)\n",
    "    \n",
    "    print(\"Model Performance Comparison:\")\n",
    "    print(results_df)\n",
    "    \n",
    "    # Plot model comparison\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    metrics = ['R²', 'RMSE', 'MAE', 'MSE']\n",
    "    for i, metric in enumerate(metrics):\n",
    "        ax = axes[i//2, i%2]\n",
    "        results_df[metric].plot(kind='bar', ax=ax)\n",
    "        ax.set_title(f'{metric} Comparison')\n",
    "        ax.set_ylabel(metric)\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Identify best model\n",
    "    best_model_name = results_df.index[0]\n",
    "    best_model = models[best_model_name]\n",
    "    print(f\"\\nBest performing model: {best_model_name}\")\n",
    "    print(f\"R² Score: {results_df.loc[best_model_name, 'R²']:.3f}\")\n",
    "else:\n",
    "    print(\"No model results to display.\")\n",
    "    best_model = best_model_name = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c136f9",
   "metadata": {},
   "source": [
    "## 5. Model Interpretation {#interpretation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e89ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for best model\n",
    "if best_model is not None and hasattr(best_model, 'feature_importances_'):\n",
    "    # Get feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Plot top 10 features\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    top_features = feature_importance.head(10)\n",
    "    sns.barplot(data=top_features, x='importance', y='feature')\n",
    "    plt.title(f'Top 10 Feature Importance - {best_model_name}')\n",
    "    plt.xlabel('Importance Score')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Top 5 Most Important Features for {best_model_name}:\")\n",
    "    print(feature_importance.head())\n",
    "\n",
    "elif best_model is not None and hasattr(best_model, 'coef_'):\n",
    "    # For linear models, show coefficients\n",
    "    coefficients = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'coefficient': best_model.coef_\n",
    "    }).sort_values('coefficient', key=abs, ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    top_coef = coefficients.head(10)\n",
    "    sns.barplot(data=top_coef, x='coefficient', y='feature')\n",
    "    plt.title(f'Top 10 Feature Coefficients - {best_model_name}')\n",
    "    plt.xlabel('Coefficient Value')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Top 5 Features by Coefficient Magnitude for {best_model_name}:\")\n",
    "    print(coefficients.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f43387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction vs Actual scatter plot\n",
    "if best_model is not None and X_test is not None:\n",
    "    # Make predictions\n",
    "    if 'Linear' in best_model_name or 'Ridge' in best_model_name:\n",
    "        y_pred_best = best_model.predict(X_test_scaled)\n",
    "    else:\n",
    "        y_pred_best = best_model.predict(X_test)\n",
    "    \n",
    "    # Create scatter plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(y_test, y_pred_best, alpha=0.6)\n",
    "    \n",
    "    # Add perfect prediction line\n",
    "    min_val = min(min(y_test), min(y_pred_best))\n",
    "    max_val = max(max(y_test), max(y_pred_best))\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')\n",
    "    \n",
    "    plt.xlabel('Actual Success Score')\n",
    "    plt.ylabel('Predicted Success Score')\n",
    "    plt.title(f'Prediction vs Actual - {best_model_name}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate residuals\n",
    "    residuals = y_test - y_pred_best\n",
    "    \n",
    "    # Residual plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_pred_best, residuals, alpha=0.6)\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.xlabel('Predicted Success Score')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.title(f'Residual Plot - {best_model_name}')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Residual Statistics:\")\n",
    "    print(f\"Mean: {residuals.mean():.4f}\")\n",
    "    print(f\"Std: {residuals.std():.4f}\")\n",
    "    print(f\"Mean Absolute Residual: {abs(residuals).mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d508f55",
   "metadata": {},
   "source": [
    "## 6. Prediction and Visualization {#prediction}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bbd0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prediction function for new locations\n",
    "def predict_restaurant_success(latitude, longitude, price_level=2, model=best_model, scaler_obj=scaler):\n",
    "    \"\"\"\n",
    "    Predict success score for a new restaurant location\n",
    "    \"\"\"\n",
    "    if model is None:\n",
    "        return \"No trained model available\"\n",
    "    \n",
    "    # Create feature vector (simplified - would need actual feature engineering)\n",
    "    # This is a placeholder implementation\n",
    "    features = np.array([[latitude, longitude, price_level] + [0] * (len(X.columns) - 3)])\n",
    "    features = features[:, :len(X.columns)]  # Ensure correct number of features\n",
    "    \n",
    "    # Scale if needed\n",
    "    if 'Linear' in best_model_name or 'Ridge' in best_model_name:\n",
    "        features_scaled = scaler_obj.transform(features)\n",
    "        prediction = model.predict(features_scaled)[0]\n",
    "    else:\n",
    "        prediction = model.predict(features)[0]\n",
    "    \n",
    "    return max(0, min(1, prediction))  # Ensure prediction is between 0 and 1\n",
    "\n",
    "# Example predictions for different locations in Vancouver\n",
    "if best_model is not None:\n",
    "    example_locations = [\n",
    "        (49.2827, -123.1207, \"Downtown Vancouver\"),\n",
    "        (49.2606, -123.2460, \"Kitsilano\"),\n",
    "        (49.2488, -123.1003, \"Mount Pleasant\")\n",
    "    ]\n",
    "    \n",
    "    print(\"Example Success Score Predictions:\")\n",
    "    for lat, lon, name in example_locations:\n",
    "        pred_score = predict_restaurant_success(lat, lon)\n",
    "        print(f\"{name}: {pred_score:.3f}\")\n",
    "else:\n",
    "    print(\"No model available for predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c75fb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple heat map visualization\n",
    "if best_model is not None and 'latitude' in df_restaurants.columns:\n",
    "    # Create a grid of predictions across Vancouver\n",
    "    lat_min, lat_max = df_restaurants['latitude'].min(), df_restaurants['latitude'].max()\n",
    "    lon_min, lon_max = df_restaurants['longitude'].min(), df_restaurants['longitude'].max()\n",
    "    \n",
    "    # Create grid\n",
    "    lat_range = np.linspace(lat_min, lat_max, 20)\n",
    "    lon_range = np.linspace(lon_min, lon_max, 20)\n",
    "    \n",
    "    # Generate predictions for grid points\n",
    "    grid_predictions = []\n",
    "    for lat in lat_range:\n",
    "        for lon in lon_range:\n",
    "            pred = predict_restaurant_success(lat, lon)\n",
    "            if isinstance(pred, (int, float)):\n",
    "                grid_predictions.append([lat, lon, pred])\n",
    "    \n",
    "    if grid_predictions:\n",
    "        grid_df = pd.DataFrame(grid_predictions, columns=['latitude', 'longitude', 'predicted_success'])\n",
    "        \n",
    "        # Create heat map visualization\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        \n",
    "        # Scatter plot with predictions\n",
    "        scatter = plt.scatter(grid_df['longitude'], grid_df['latitude'], \n",
    "                            c=grid_df['predicted_success'], cmap='RdYlGn', \n",
    "                            s=100, alpha=0.7, edgecolors='black', linewidth=0.5)\n",
    "        \n",
    "        # Overlay actual restaurant locations\n",
    "        plt.scatter(df_restaurants['longitude'], df_restaurants['latitude'], \n",
    "                   c='blue', s=20, alpha=0.5, label='Existing Restaurants')\n",
    "        \n",
    "        plt.colorbar(scatter, label='Predicted Success Score')\n",
    "        plt.title('Restaurant Success Prediction Heat Map - Vancouver')\n",
    "        plt.xlabel('Longitude')\n",
    "        plt.ylabel('Latitude')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"Heat map created with {len(grid_predictions)} prediction points\")\n",
    "    else:\n",
    "        print(\"Could not generate grid predictions\")\n",
    "else:\n",
    "    print(\"Heat map visualization not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25d93fd",
   "metadata": {},
   "source": [
    "## Summary and Conclusions\n",
    "\n",
    "This notebook demonstrated the complete machine learning pipeline for predicting restaurant success in Vancouver:\n",
    "\n",
    "1. **Data Integration**: Combined multiple data sources including business licenses, Yelp reviews, demographics, and transit data\n",
    "2. **Feature Engineering**: Created meaningful predictors like competitive density, transit accessibility, and affordability mismatch\n",
    "3. **Model Training**: Tested multiple algorithms and selected the best performer\n",
    "4. **Model Interpretation**: Analyzed feature importance and model predictions\n",
    "5. **Visualization**: Created geographic visualizations and prediction heat maps\n",
    "\n",
    "### Key Findings:\n",
    "- [To be filled based on actual model results]\n",
    "- Most important factors for restaurant success\n",
    "- Geographic patterns in success predictions\n",
    "- Model performance and limitations\n",
    "\n",
    "### Next Steps:\n",
    "1. Collect more comprehensive real-world data\n",
    "2. Implement advanced feature engineering\n",
    "3. Explore ensemble methods and deep learning approaches\n",
    "4. Deploy model as a web application for real-time predictions\n",
    "5. Validate predictions with actual business outcomes"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
